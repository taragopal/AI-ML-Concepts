Deploy an AI model on Azure Kubernetes Service (AKS) with the AI toolchain operator add-on

Why there is a need? 

Self-hosting LLMs on Kubernetes is gaining momentum among organizations with inference workloads at scale, such as batch processing, chatbots, agents, and AI-driven applications. These organizations often have
access to commercial-grade GPUs and are seeking alternatives to costly per-token API pricing models, which can quickly scale out of control. Many also require the ability to fine-tune or customize their models,
a capability typically restricted by closed-source API providers. Additionally, companies handling sensitive or proprietary data - especially in regulated sectors such as finance, healthcare, or defense - 
prioritize self-hosting to maintain strict control over data and prevent exposure through third-party systems.

To address these needs: 
Use the AI toolchain operator add-on to efficiently self-host large language models on Kubernetes, reducing costs and resource complexity, enhancing customization, and maintaining full control over your data.

Kubernetes AI Toolchain Operator (KAITO), a Cloud Native Computing Foundation (CNCF) Sandbox project, simplifies the process of deploying and managing open-source LLM workloads on Kubernetes. KAITO integrates
with vLLM, a high-throughput inference engine designed to serve large language models efficiently. vLLM as an inference engine helps reduce memory and GPU requirements without significantly compromising accuracy.

These AI toolchain operator managed add-on offers a modular, plug-and-play setup that allows teams to quickly deploy models and expose them via production-ready APIs. It includes built-in features like 
OpenAI-compatible APIs, prompt formatting, and streaming response support.


Enable the AI toolchain operator add-on on an AKS cluster
==========================================================

Create an AKS cluster with the AI toolchain operator add-on enabled
--------------------------------------------------------------------
Create a new Azure RG: 
az group create --name $AZURE_RESOURCE_GROUP --location $AZURE_LOCATION

Create an AKS cluster with the AI toolchain operator add-on enabled: 
az aks create --location $AZURE_LOCATION --resource-group $AZURE_RESOURCE_GROUP --name $CLUSTER_NAME --enable-ai-toolchain-operator --enable-oidc-issuer --generate-ssh-keys

On an existing AKS cluster, you can enable the AI toolchain operator add-on: 
az aks update --name $CLUSTER_NAME --resource-group $AZURE_RESOURCE_GROUP --enable-ai-toolchain-operator --enable-oidc-issuer

CONNECT to the CLUSTER
----------------------
Configure kubectl to connect to your cluster using the az aks get-credentials command: 
az aks get-credentials --resource-group $AZURE_RESOURCE_GROUP --name $CLUSTER_NAME


Verify the connection to your cluster using the kubectl get command: kubectl get nodes

Deploy a default hosted AI model
---------------------------------

Deploy the Phi-4-mini instruct model preset for inference from the KAITO model repository using the kubectl apply command:
kubectl apply -f https://raw.githubusercontent.com/kaito-project/kaito/refs/heads/main/examples/inference/kaito_workspace_phi_4_mini.yaml

Track the live resource changes in your workspace: kubectl get workspace workspace-phi-4-mini -w

Check your inference service and get the service IP address: export SERVICE_IP=$(kubectl get svc workspace-phi-4-mini -o jsonpath='{.spec.clusterIP}')

Test the Phi-4-mini instruct inference service with a sample input of your choice using the OpenAI chat completions API format: (https://platform.openai.com/docs/api-reference/chat)
kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/v1/completions -H "Content-Type: application/json" -d '{
        "model": "phi-4-mini-instruct",
        "prompt": "How should I dress for the weather today?",
        "max_tokens": 10
       }'


Deploy a custom or domain-specific LLM
-------------------------------------------
Open-source LLMs are often trained in different contexts and domains, and the hosted model presets may not always fit the requirements of your application or data. In this case, KAITO also supports inference
deployment of newer or domain-specific language models from HuggingFace. Try out a custom model inference deployment with KAITO by following this https://learn.microsoft.com/en-us/azure/aks/kaito-custom-inference-model.


Fine-tune and deploy an AI model for inferencing on Azure Kubernetes Service (AKS) with the AI toolchain operator add-on
=========================================================================================================================

Prerequisites
--------------
The Kubernetes command-line client, kubectl, installed and configured. For more information, see Install kubectl.
Configure Azure Container Registry (ACR) integration of a new or existing ACR with your AKS cluster.
Install the AI toolchain operator add-on on your AKS cluster.
If you already have the AI toolchain operator add-on installed, update your AKS cluster to the latest version to run KAITO v0.3.1+ and ensure that the AI toolchain operator add-on feature flag is enabled.


Export Environment Variables: 
----------------------------
ACR_NAME="myACRname"
ACR_USERNAME="myACRusername"
REPOSITORY="myRepository"
VERSION="repositoryVersion'
ACR_PASSWORD=$(az acr token create --name $ACR_USERNAME --registry $ACR_NAME --expiration-in-days 10 --repository $REPOSITORY content/write content/read --query "credentials.passwords[0].value" --output tsv)

Create a new secret for your private registry
---------------------------------------------
kubectl create secret docker-registry myregistrysecret --docker-server=$ACR_NAME.azurecr.io --docker-username=$ACR_USERNAME --docker-password=$ACR_PASSWORD

Fine-tune an AI model
------------------------
Here we will fine-tune the Phi-3-mini small language model using the qLoRA tuning method by applying the following Phi-3-mini KAITO fine-tuning workspace CRD:

YAML üç† 
apiVersion: kaito.sh/v1alpha1
kind: Workspace
metadata:
     name: workspace-tuning-phi-3-mini
resource:
     instanceType: "Standard_NC24ads_A100_v4"
     labelSelector:
          matchLabels:
                apps: tuning-phi-3-mini-pycoder
tuning:
     preset:
         name: phi3mini128kinst
  method: qlora
  input:
      urls: 
          - ‚ÄúmyDatasetURL‚Äù
  output:
      image: ‚Äú$ACR_NAME.azurecr.io/$REPOSITORY:$VERSION‚Äù
      imagePushSecret: myregistrysecret

Reference:   https://github.com/kaito-project/kaito/tree/main

1. Apply the KAITO fine-tuning workspace CRD using the kubectl apply command: kubectl apply workspace-tuning-phi-3-mini.yaml

2. Track the readiness of your GPU resources, fine-tuning job, and workspace using the kubectl get workspace command: kubectl get workspace -w

Output:
NAME                         INSTANCE                  RESOURCE READY  INFERENCE READY  JOB STARTED  WORKSPACE SUCCEEDED  AGE
workspace-tuning-phi-3-mini  Standard_NC24ads_A100_v4  True                             True                              3m 45s

3. Check the status of your fine-tuning job pods using the kubectl get pods command: kubectl get pods

Deploy the fine-tuned model for inferencing
---------------------------------------------
Now, you use the Phi-3-mini adapter image created in the previous section for a new inferencing deployment with this model. 
The KAITO inference workspace CRD below consists of the following resources and adapter(s) to deploy on your AKS cluster:

apiVersion: kaito.sh/v1alpha1
kind: Workspace
metadata:
  name: workspace-phi-3-mini-adapter
resource:
  instanceType: "Standard_NC6s_v3"
  labelSelector:
    matchLabels:
      apps: phi-3-adapter
inference:
  preset:
    name: ‚Äúphi-3-mini-128k-instruct‚Äú
  adapters:
    -source:
       name: kubernetes-adapter
       image: $ACR_NAME.azurecr.io/$REPOSITORY:$VERSION
       imagePullSecrets:
             - myregistrysecret
     strength: ‚Äú1.0‚Äù

1. Apply the KAITO inference workspace CRD using the kubectl apply command: kubectl apply -f workspace-phi-3-mini-adapter.yaml

2. Track the readiness of your GPU resources, inference server, and workspace using the kubectl get workspace command: kubectl get workspace -w

Output:
NAME                          INSTANCE          RESOURCE READY  INFERENCE READY  JOB STARTED  WORKSPACE SUCCEEDED  AGE
workspace-phi-3-mini-adapter  Standard_NC6s_v3  True            True                          True                 5m 47s

3. Check the status of your inferencing workload pods using the kubectl get pods command: kubectl get pods
** It might take several minutes for your pods to show the Running status.

Test the model inference service endpoint
------------------------------------------

1. Check your model inferencing service and retrieve the service IP address using the kubectl get svc command.
export SERVICE_IP=$(kubectl get svc workspace-phi-3-mini-adapter -o jsonpath=‚Äô{.spec.clusterIP}‚Äô)

2. Run your fine-tuned Phi-3-mini model with a sample input of your choice using the kubectl run command. The following example asks the generative AI model, "What is AKS?":
kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/chat -H "accept: application/json" -H "Content-Type: application/json" -d "{\"prompt\":\"What is AKS?\"}"

3. Output :
"Kubernetes on Azure" is the official name.
https://learn.microsoft.com/en-us/azure/aks/ ...

Cleanup Resources
------------------
kubectl delete workspace workspace-tuning-phi-3-mini
kubectl delete workspace workspace-phi-3-mini-adapter













