One of the core features of the Semantic Kernel SDK is that it allows developers to combine native code with the power of large language models (LLMs). Plugins encapsulate your functions and allow them to be used
by the AI. This enables your AI the ability to perform actions that it wouldn't otherwise be able to do.

An example: 
Suppose you want to create a smart traffic routing system in a city. You can add plugin functions to your kernel that can interact with a daily passengers, number of fleets to a source and destination, and modes
of transportation, weather disruptions, local law and order information from police, possible events and large gathering and aggregate all these to deploy a traffic routing system.

Creating a plugin
Â©ï¸Â©ï¸Â©ï¸ðŸš€ðŸš€ðŸš€Â©ï¸Â©ï¸Â©ï¸
For the Semantic Kernel to correctly route requests to your functions, the plugins you create must include details that describe the function's behavior. The details need to be written in a way that can be understood
by the AI. The function's input, output and side effects should be described so that the AI can use the function.

To create a plugin function in Python, you use the @kernel_function decorator with a description parameter. The return type is specified in the function signature (for example, -> dict | None). Hereâ€™s a minimal 
example:

ðŸðŸPYTHONðŸðŸ
from semantic_kernel import kernel_function, KernelPlugin

class TaskManagementPlugin(KernelPlugin):
    @kernel_function(name="complete_task", description="Marks a task as completed by its ID.")
    def complete_task(self, id: int) -> dict | None:
        # ...complete the task logic...
        pass
# In Python, plugin functions use decorators such as @kernel_function to provide metadata for the Semantic Kernel. Chat history is typically represented as a list of dictionaries, each with a role and content key.

#below call the function to register the plugin 
from semantic_kernel import Kernel

kernel = Kernel()
kernel.add_plugin(TaskManagementPlugin(), "TaskManagement")

arguments = {"id": 1}
updated_task = await kernel.invoke("TaskManagement.complete_task", arguments)

Invoke functions automatically
ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©

Semantic Kernel SDK enables the LLM to automatically invoke your plugin functions. Automatically invoking functions allows your application to respond more intelligently to user input. Instead of requiring explicit commands to trigger specific actions, the AI can determine the appropriate function to call based on the user's request. This enhances the user experience by making interactions more natural and reducing the need for precise instructions.

To enable functions to be invoked automatically:

Set function_call="auto" in the OpenAIChatCompletionSettings object. This allows user prompts to automatically trigger your plugin functions based on the LLM's understanding of the user's intent.

Suppose the TaskManagementPlugin contains a GetCriticalTasks function:

ðŸðŸPYTHONðŸðŸ
from semantic_kernel import kernel_function, KernelPlugin

class TaskManagementPlugin(KernelPlugin):
    @kernel_function(name="get_critical_tasks", description="Gets a list of all tasks marked as 'Critical' priority.")
    def get_critical_tasks(self) -> list:
        # ...return critical tasks...
        pass

#The user could trigger this function with a prompt to the LLM. For example:
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletionSettings
settings = OpenAIChatCompletionSettings(function_call="auto")
history = [{"role": "user", "content": "What are all of the critical tasks?"}]
chat_completion_service = kernel.get_service("chat_completion")

result = await chat_completion_service.get_chat_message_content(
    history=history,
    execution_settings=settings,
    kernel=kernel
)
print("Assistant:", result)

#Assistant: The only critical task is "Fix login bug". The task is currently in the "To Do" status and its description is "Resolve the issue with login sessions timing out".

Make plugins AI-friendly
ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©
To enhance the LLM's ability to understand and utilize your plugin functions, consider the following guidelines:

Use descriptive and concise function names
Names that clearly convey the function's purpose will help the model understand when to call the function. Avoid abbreviations or acronyms. Use the description parameter in the @kernel_function decorator to provide context and instructions when necessary.

Minimize function parameters
Limit the number of function parameters and use primitive types whenever possible. This reduces token consumption and simplifies the function signature.

Name function parameters clearly
Use descriptive parameter names that clarify their purpose. Avoid abbreviations or acronyms.


























