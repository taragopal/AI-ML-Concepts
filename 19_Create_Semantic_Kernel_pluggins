One of the core features of the Semantic Kernel SDK is that it allows developers to combine native code with the power of large language models (LLMs). Plugins encapsulate your functions and allow them to be used
by the AI. This enables your AI the ability to perform actions that it wouldn't otherwise be able to do.

An example: 
Suppose you want to create a smart traffic routing system in a city. You can add plugin functions to your kernel that can interact with a daily passengers, number of fleets to a source and destination, and modes
of transportation, weather disruptions, local law and order information from police, possible events and large gathering and aggregate all these to deploy a traffic routing system.

Creating a plugin
©️©️©️🚀🚀🚀©️©️©️
For the Semantic Kernel to correctly route requests to your functions, the plugins you create must include details that describe the function's behavior. The details need to be written in a way that can be understood
by the AI. The function's input, output and side effects should be described so that the AI can use the function.

To create a plugin function in Python, you use the @kernel_function decorator with a description parameter. The return type is specified in the function signature (for example, -> dict | None). Here’s a minimal 
example:

🐍🐍PYTHON🐍🐍
from semantic_kernel import kernel_function, KernelPlugin

class TaskManagementPlugin(KernelPlugin):
    @kernel_function(name="complete_task", description="Marks a task as completed by its ID.")
    def complete_task(self, id: int) -> dict | None:
        # ...complete the task logic...
        pass
# In Python, plugin functions use decorators such as @kernel_function to provide metadata for the Semantic Kernel. Chat history is typically represented as a list of dictionaries, each with a role and content key.

#below call the function to register the plugin 
from semantic_kernel import Kernel

kernel = Kernel()
kernel.add_plugin(TaskManagementPlugin(), "TaskManagement")

arguments = {"id": 1}
updated_task = await kernel.invoke("TaskManagement.complete_task", arguments)

Invoke functions automatically
🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩

Semantic Kernel SDK enables the LLM to automatically invoke your plugin functions. Automatically invoking functions allows your application to respond more intelligently to user input. Instead of requiring
explicit commands to trigger specific actions, the AI can determine the appropriate function to call based on the user's request. This enhances the user experience by making interactions more natural and 
reducing the need for precise instructions.

To enable functions to be invoked automatically:

Set function_call="auto" in the OpenAIChatCompletionSettings object. This allows user prompts to automatically trigger your plugin functions based on the LLM's understanding of the user's intent.

Suppose the TaskManagementPlugin contains a GetCriticalTasks function:

🐍🐍PYTHON🐍🐍
from semantic_kernel import kernel_function, KernelPlugin

class TaskManagementPlugin(KernelPlugin):
    @kernel_function(name="get_critical_tasks", description="Gets a list of all tasks marked as 'Critical' priority.")
    def get_critical_tasks(self) -> list:
        # ...return critical tasks...
        pass

#The user could trigger this function with a prompt to the LLM. For example:
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletionSettings
settings = OpenAIChatCompletionSettings(function_call="auto")
history = [{"role": "user", "content": "What are all of the critical tasks?"}]
chat_completion_service = kernel.get_service("chat_completion")

result = await chat_completion_service.get_chat_message_content(
    history=history,
    execution_settings=settings,
    kernel=kernel
)
print("Assistant:", result)

#Assistant: The only critical task is "Fix login bug". The task is currently in the "To Do" status and its description is "Resolve the issue with login sessions timing out".

Make plugins AI-friendly
🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩🤩
To enhance the LLM's ability to understand and utilize your plugin functions, consider the following guidelines:

Use descriptive and concise function names: Names that clearly convey the function's purpose will help the model understand when to call the function. Avoid abbreviations or acronyms. Use the description 
parameter in the @kernel_function decorator to provide context and instructions when necessary.

Minimize function parameters: Limit the number of function parameters and use primitive types whenever possible. This reduces token consumption and simplifies the function signature.

Name function parameters clearly: Use descriptive parameter names that clarify their purpose. Avoid abbreviations or acronyms.

Function Advertising: Function advertising means providing specific functions to the AI model so it can invoke them. By default, all of the functions from registered plugins are available unless a list of 
functions is explicitly provided. You can advertise all registered functions, limit availability to specific ones, or disable function calling entirely to rely solely on the language model's reasoning.

Advertising All Functions: By using AddFromType, all of the functions from the specified classes are registered to the kernel. The AI model can then automatically choose from any of these functions based on the
prompt.

When you register plugin classes with the kernel, all of their functions become available to the AI model. The model can then automatically choose from any of these functions based on the prompt. In Python,
use add_plugin to register your plugin classes and their functions.

---------------------------------------------------------------------------------------------------------------------
Function Advertising
Function advertising means providing specific functions to the AI model so it can invoke them. By default, all of the functions from registered plugins are available unless a list of functions is explicitly
provided. You can advertise all registered functions, limit availability to specific ones, or disable function calling entirely to rely solely on the language model's reasoning.

Advertising All Functions
By using AddFromType, all of the functions from the specified classes are registered to the kernel. The AI model can then automatically choose from any of these functions based on the prompt.

When you register plugin classes with the kernel, all of their functions become available to the AI model. The model can then automatically choose from any of these functions based on the prompt. In Python,
use add_plugin to register your plugin classes and their functions.
🐍🐍PYTHON🐍🐍
-------------------------
import os
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIPromptExecutionSettings
from semantic_kernel.functions.kernel_function_decorator import kernel_function
from semantic_kernel.contents.chat_history import ChatHistory

class WeatherForecastUtils:
    @kernel_function(name="GetWeatherForCity", description="Gets the weather for a given city.")
    def get_weather_for_city(self, city: str) -> str:
        return "Sunny"  # Stub for demo

class DateTimeUtils:
    @kernel_function(name="GetCurrentUtcDateTime", description="Gets the current UTC date and time.")
    def get_current_utc_date_time(self) -> str:
        import datetime
        return datetime.datetime.utcnow().isoformat()

deployment_name = os.getenv("DEPLOYMENT_NAME")
endpoint = os.getenv("PROJECT_ENDPOINT")
api_key = os.getenv("PROJECT_KEY")

kernel = Kernel()
chat_service = AzureChatCompletion(
    deployment_name=deployment_name,
    endpoint=endpoint,
    api_key=api_key
)
kernel.add_service(chat_service, "chat_completion")
kernel.add_plugin(WeatherForecastUtils(), "WeatherForecastUtils")
kernel.add_plugin(DateTimeUtils(), "DateTimeUtils")

settings = OpenAIPromptExecutionSettings()
chat_history = ChatHistory()
chat_history.add_user_message("What is the likely color of the sky in Boston?")

result = chat_service.get_chat_message_content(
    chat_history=chat_history,
    kernel=kernel,
    settings=settings
)
print(result)


Advertising Selected Functions
---------------------------------
Instead of making all functions available, you can explicitly select and advertise just the required ones. This approach offers more control and limits the functions the model can use.
You control which functions are available by only registering the plugins you want the model to access. The SDK doesn't currently support restricting advertised functions directly in settings.

🐍🐍PYTHON🐍🐍
-----------------
# As of SK 1.31, Python SDK doe    s not support restricting advertised functions directly in settings.
# The closest approach is to only register the plugins you want available.

kernel = Kernel()
chat_service = AzureChatCompletion(
    deployment_name=deployment_name,
    endpoint=endpoint,
    api_key=api_key
)
kernel.add_service(chat_service, "chat_completion")
kernel.add_plugin(WeatherForecastUtils(), "WeatherForecastUtils")
# Only add DateTimeUtils if you want it available:
# kernel.add_plugin(DateTimeUtils(), "DateTimeUtils")

settings = OpenAIPromptExecutionSettings()
chat_history = ChatHistory()
chat_history.add_user_message("What is the likely color of the sky in Boston?")

result = chat_service.get_chat_message_content(
    chat_history=chat_history,
    kernel=kernel,
    settings=settings
)
print(result)


Disabling Function Calling
--------------------------------
You can prevent the AI model from invoking any functions, forcing it to rely solely on its language model capabilities to process the prompt.
In Python, don't register any plugins with the kernel to disable function calling.

🐍🐍PYTHON🐍🐍
-----------------
# In Python, you can disable function calling by not registering any plugins,
# or by using settings that prevent tool/function use if supported in your SDK version.
# As of SK 1.31, you can use the following pattern:

settings = OpenAIPromptExecutionSettings()  # No plugins registered, so no functions available

kernel = Kernel()
chat_service = AzureChatCompletion(
    deployment_name=deployment_name,
    endpoint=endpoint,
    api_key=api_key
)
kernel.add_service(chat_service, "chat_completion")
# Do NOT add any plugins

chat_history = ChatHistory()
chat_history.add_user_message("What is the likely color of the sky in Boston?")

result = chat_service.get_chat_message_content(
    chat_history=chat_history,
    kernel=kernel,
    settings=settings
)
print(result)

Using Function Choice Behaviors
The Semantic Kernel SDK provides several ways to configure how functions are advertised and selected for invocation by the AI model:

Auto: The model can choose from zero or more functions.
Required: The model is encouraged or required to choose at least one function.
None: The model can't choose any functions.




















