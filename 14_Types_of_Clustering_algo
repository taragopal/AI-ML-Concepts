Most common types of clustering algorithms: 

1. Centroid-Based Clustering (Partitioning Methods)
-----------------------------------------------------
This is the most popular and widely used category. These algorithms partition the data into a pre-specified number of clusters (k). They work by assigning data points to clusters based on their proximity to a cluster's center (its centroid).


Principle: Clusters are represented by their central vectors or centroids, and each point belongs to the cluster with the closest centroid.

Strengths: Simple to understand, fast, and computationally efficient, making them suitable for large datasets.

Weaknesses: They require we to specify the number of clusters (k) in advance, which can be challenging. They also assume that clusters are spherical and of roughly equal size, and they are sensitive to outliers.

Example Algorithms: K-Means, K-Medoids.

Using Python and the popular scikit-learn library, we can easily implement the top clustering algorithms. Here are code examples for one algorithm from each of the four main categories.

Centroid-Based Clustering: K-Means
K-Means is a classic example of a centroid-based algorithm. we must specify the number of clusters (n_clusters) in advance.

#Sample code
import numpy as np
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 1. Generate some sample data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# 2. Instantiate the K-Means model
# n_clusters is the number of clusters you want to find
kmeans = KMeans(n_clusters=4, random_state=0, n_init='auto')

# 3. Fit the model to the data and predict the cluster labels
kmeans.fit(X)
labels = kmeans.labels_

# 4. Get the final cluster centers
centers = kmeans.cluster_centers_

# Print the results
print("Cluster labels:\n", labels)
print("\nCluster centers:\n", centers)


2. Density-Based Clustering
------------------------------
These algorithms define clusters as contiguous regions of high data point density, separated by areas of low density. This approach allows them to find clusters of arbitrary shapes and sizes.


Principle: Clusters are formed by dense regions of data points. Points in low-density areas are considered noise or outliers.


Strengths: Can find clusters of non-spherical shapes, is robust to outliers, and does not require we to specify the number of clusters beforehand.

Weaknesses: Struggles with clusters of varying densities and performs poorly on high-dimensional data.

Example Algorithms: DBSCAN (Density-Based Spatial Clustering of Applications with Noise), OPTICS.

DBSCAN is excellent for finding clusters of arbitrary shapes and for identifying outliers. It does not require you to specify the number of clusters.

#Sample code
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# 1. Generate non-globular sample data (a challenge for K-Means)
X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)

# 2. Instantiate the DBSCAN model
# eps: The maximum distance between two samples for one to be considered a neighbor of the other.
# min_samples: The number of samples in a neighborhood for a point to be considered as a core point.
dbscan = DBSCAN(eps=0.15, min_samples=5)

# 3. Fit the model and get the cluster labels
# A label of -1 indicates noise/outliers
labels = dbscan.fit_predict(X)

# Print the results
print("Cluster labels (Note: -1 is noise):\n", labels)
print("\nNumber of clusters found:", len(np.unique(labels[labels != -1])))
print("Number of noise points:", np.sum(labels == -1))

3. Hierarchical Clustering
------------------------------
These algorithms create a tree-like hierarchy of clusters. They do not require a pre-specified number of clusters; instead, the user can decide on the number of clusters by cutting the tree (dendrogram) at a certain level.

Principle: Based on the idea that nearby objects are more related than objects farther away. Clusters are formed by either merging smaller clusters or splitting a large one.

Strengths: No need to specify the number of clusters, and the hierarchical structure (dendrogram) can provide valuable insights into the data's nested relationships.

Weaknesses: Computationally expensive and not suitable for very large datasets due to its high time and space complexity. It is also sensitive to noise and outliers.

Example Algorithms: Agglomerative (bottom-up approach) and Divisive (top-down approach). Agglomerative Clustering builds a hierarchy of clusters. You can either specify the number of clusters or a distance threshold to cut the tree.

#Sample code
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs

# 1. Generate some sample data
X, _ = make_blobs(n_samples=100, centers=3, cluster_std=0.5, random_state=0)

# 2. Instantiate the AgglomerativeClustering model
# n_clusters: The number of clusters to find.
# linkage: The criterion used to merge clusters (e.g., 'ward', 'complete', 'average').
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')

# 3. Fit the model and get the cluster labels
labels = agg_clustering.fit_predict(X)

# Print the results
print("Cluster labels:\n", labels)

4. Distribution-Based Clustering
-------------------------------------
These algorithms assume that the data points are generated from a mixture of probability distributions (e.g., Gaussian, Poisson). Clusters are defined as these distributions, and each data point belongs to the cluster with the highest probability.

Principle: The data is modeled as a mixture of probability distributions, where each distribution represents a cluster.

Strengths: Can handle clusters of different sizes and elliptical shapes. It provides a more probabilistic and flexible view of cluster assignments.

Weaknesses: The performance is highly dependent on the initial assumption about the underlying distribution, and it can be computationally intensive.

Example Algorithm: Gaussian Mixture Model (GMM). GMMs are a probabilistic approach that models the data as a mixture of multiple Gaussian distributions.

#Sample Python code
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# 1. Generate some sample data
X, _ = make_blobs(n_samples=200, centers=4, cluster_std=0.8, random_state=0)

# 2. Instantiate the GMM model
# n_components: The number of Gaussian mixture components (clusters) to find.
# covariance_type: The type of covariance parameter to use ('full' is a common default).
gmm = GaussianMixture(n_components=4, random_state=0)

# 3. Fit the model to the data
gmm.fit(X)

# 4. Predict the cluster labels for each data point
labels = gmm.predict(X)

# Print the results
print("Cluster labels:\n", labels)


5. Other Types

There are other, more specialized types of clustering algorithms that fit into different paradigms:

Fuzzy Clustering: Allows data points to belong to multiple clusters with a degree of membership, typically between 0 and 1.

Grid-Based Clustering: Divides the data space into a grid of cells and then performs clustering on the grid, making it very efficient for large datasets.

Constraint-Based Clustering: Incorporates user-specified constraints into the clustering process, such as "these two points must be in the same cluster."

Subspace Clustering: Designed for high-dimensional data, it finds clusters in a subset of the original features.
