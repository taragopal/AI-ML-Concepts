Deploy an AI model on Azure Kubernetes Service (AKS) with the AI toolchain operator add-on

Why there is a need? 

Self-hosting LLMs on Kubernetes is gaining momentum among organizations with inference workloads at scale, such as batch processing, chatbots, agents, and AI-driven applications. These organizations often have
access to commercial-grade GPUs and are seeking alternatives to costly per-token API pricing models, which can quickly scale out of control. Many also require the ability to fine-tune or customize their models,
a capability typically restricted by closed-source API providers. Additionally, companies handling sensitive or proprietary data - especially in regulated sectors such as finance, healthcare, or defense - 
prioritize self-hosting to maintain strict control over data and prevent exposure through third-party systems.


To address these needs: 
Use the AI toolchain operator add-on to efficiently self-host large language models on Kubernetes, reducing costs and resource complexity, enhancing customization, and maintaining full control over your data.

Kubernetes AI Toolchain Operator (KAITO), a Cloud Native Computing Foundation (CNCF) Sandbox project, simplifies the process of deploying and managing open-source LLM workloads on Kubernetes. KAITO integrates
with vLLM, a high-throughput inference engine designed to serve large language models efficiently. vLLM as an inference engine helps reduce memory and GPU requirements without significantly compromising accuracy.

These AI toolchain operator managed add-on offers a modular, plug-and-play setup that allows teams to quickly deploy models and expose them via production-ready APIs. It includes built-in features like 
OpenAI-compatible APIs, prompt formatting, and streaming response support.


Enable the AI toolchain operator add-on on an AKS cluster
==========================================================

Create an AKS cluster with the AI toolchain operator add-on enabled
--------------------------------------------------------------------
Create a new Azure RG: 
az group create --name $AZURE_RESOURCE_GROUP --location $AZURE_LOCATION

Create an AKS cluster with the AI toolchain operator add-on enabled: 
az aks create --location $AZURE_LOCATION --resource-group $AZURE_RESOURCE_GROUP --name $CLUSTER_NAME --enable-ai-toolchain-operator --enable-oidc-issuer --generate-ssh-keys

On an existing AKS cluster, you can enable the AI toolchain operator add-on: 
az aks update --name $CLUSTER_NAME --resource-group $AZURE_RESOURCE_GROUP --enable-ai-toolchain-operator --enable-oidc-issuer

CONNECT to the CLUSTER
----------------------
Configure kubectl to connect to your cluster using the az aks get-credentials command: 
az aks get-credentials --resource-group $AZURE_RESOURCE_GROUP --name $CLUSTER_NAME


Verify the connection to your cluster using the kubectl get command: kubectl get nodes

Deploy a default hosted AI model
---------------------------------

Deploy the Phi-4-mini instruct model preset for inference from the KAITO model repository using the kubectl apply command:
kubectl apply -f https://raw.githubusercontent.com/kaito-project/kaito/refs/heads/main/examples/inference/kaito_workspace_phi_4_mini.yaml

Track the live resource changes in your workspace: kubectl get workspace workspace-phi-4-mini -w

Check your inference service and get the service IP address: export SERVICE_IP=$(kubectl get svc workspace-phi-4-mini -o jsonpath='{.spec.clusterIP}')

Test the Phi-4-mini instruct inference service with a sample input of your choice using the OpenAI chat completions API format: (https://platform.openai.com/docs/api-reference/chat)
kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/v1/completions -H "Content-Type: application/json" -d '{
        "model": "phi-4-mini-instruct",
        "prompt": "How should I dress for the weather today?",
        "max_tokens": 10
       }'


Deploy a custom or domain-specific LLM
-------------------------------------------
Open-source LLMs are often trained in different contexts and domains, and the hosted model presets may not always fit the requirements of your application or data. In this case, KAITO also supports inference
deployment of newer or domain-specific language models from HuggingFace. Try out a custom model inference deployment with KAITO by following this https://learn.microsoft.com/en-us/azure/aks/kaito-custom-inference-model.


Fine-tune and deploy an AI model for inferencing on Azure Kubernetes Service (AKS) with the AI toolchain operator add-on
=========================================================================================================================

Prerequisites
--------------
The Kubernetes command-line client, kubectl, installed and configured. For more information, see Install kubectl.
Configure Azure Container Registry (ACR) integration of a new or existing ACR with your AKS cluster.
Install the AI toolchain operator add-on on your AKS cluster.
If you already have the AI toolchain operator add-on installed, update your AKS cluster to the latest version to run KAITO v0.3.1+ and ensure that the AI toolchain operator add-on feature flag is enabled.


Export Environment Variables: 
----------------------------
ACR_NAME="myACRname"
ACR_USERNAME="myACRusername"
REPOSITORY="myRepository"
VERSION="repositoryVersion'
ACR_PASSWORD=$(az acr token create --name $ACR_USERNAME --registry $ACR_NAME --expiration-in-days 10 --repository $REPOSITORY content/write content/read --query "credentials.passwords[0].value" --output tsv)

Create a new secret for your private registry
---------------------------------------------
kubectl create secret docker-registry myregistrysecret --docker-server=$ACR_NAME.azurecr.io --docker-username=$ACR_USERNAME --docker-password=$ACR_PASSWORD

Fine-tune an AI model
------------------------
Here we will fine-tune the Phi-3-mini small language model using the qLoRA tuning method by applying the following Phi-3-mini KAITO fine-tuning workspace CRD:

YAML ðŸ  
apiVersion: kaito.sh/v1alpha1
kind: Workspace
metadata:
     name: workspace-tuning-phi-3-mini
resource:
     instanceType: "Standard_NC24ads_A100_v4"
     labelSelector:
          matchLabels:
                apps: tuning-phi-3-mini-pycoder
tuning:
     preset:
         name: phi3mini128kinst
  method: qlora
  input:
      urls: 
          - â€œmyDatasetURLâ€
  output:
      image: â€œ$ACR_NAME.azurecr.io/$REPOSITORY:$VERSIONâ€
      imagePushSecret: myregistrysecret

Reference:   https://github.com/kaito-project/kaito/tree/main

1. Apply the KAITO fine-tuning workspace CRD using the kubectl apply command: kubectl apply workspace-tuning-phi-3-mini.yaml

2. Track the readiness of your GPU resources, fine-tuning job, and workspace using the kubectl get workspace command: kubectl get workspace -w

Output:
NAME                         INSTANCE                  RESOURCE READY  INFERENCE READY  JOB STARTED  WORKSPACE SUCCEEDED  AGE
workspace-tuning-phi-3-mini  Standard_NC24ads_A100_v4  True                             True                              3m 45s

3. Check the status of your fine-tuning job pods using the kubectl get pods command: kubectl get pods

Deploy the fine-tuned model for inferencing
---------------------------------------------
Now, you use the Phi-3-mini adapter image created in the previous section for a new inferencing deployment with this model. 
The KAITO inference workspace CRD below consists of the following resources and adapter(s) to deploy on your AKS cluster:

apiVersion: kaito.sh/v1alpha1
kind: Workspace
metadata:
  name: workspace-phi-3-mini-adapter
resource:
  instanceType: "Standard_NC6s_v3"
  labelSelector:
    matchLabels:
      apps: phi-3-adapter
inference:
  preset:
    name: â€œphi-3-mini-128k-instructâ€œ
  adapters:
    -source:
       name: kubernetes-adapter
       image: $ACR_NAME.azurecr.io/$REPOSITORY:$VERSION
       imagePullSecrets:
             - myregistrysecret
     strength: â€œ1.0â€

1. Apply the KAITO inference workspace CRD using the kubectl apply command: kubectl apply -f workspace-phi-3-mini-adapter.yaml

2. Track the readiness of your GPU resources, inference server, and workspace using the kubectl get workspace command: kubectl get workspace -w

Output:
NAME                          INSTANCE          RESOURCE READY  INFERENCE READY  JOB STARTED  WORKSPACE SUCCEEDED  AGE
workspace-phi-3-mini-adapter  Standard_NC6s_v3  True            True                          True                 5m 47s

3. Check the status of your inferencing workload pods using the kubectl get pods command: kubectl get pods
** It might take several minutes for your pods to show the Running status.

Test the model inference service endpoint
------------------------------------------

1. Check your model inferencing service and retrieve the service IP address using the kubectl get svc command.
export SERVICE_IP=$(kubectl get svc workspace-phi-3-mini-adapter -o jsonpath=â€™{.spec.clusterIP}â€™)

2. Run your fine-tuned Phi-3-mini model with a sample input of your choice using the kubectl run command. The following example asks the generative AI model, "What is AKS?":
kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/chat -H "accept: application/json" -H "Content-Type: application/json" -d "{\"prompt\":\"What is AKS?\"}"

3. Output :
"Kubernetes on Azure" is the official name.
https://learn.microsoft.com/en-us/azure/aks/ ...

Cleanup Resources
------------------
kubectl delete workspace workspace-tuning-phi-3-mini
kubectl delete workspace workspace-phi-3-mini-adapter


Onboard custom models for inferencing with the AI toolchain operator (KAITO) on Azure Kubernetes Service (AKS)
================================================================================================================
As an AI engineer or developer, you might have to prototype and deploy AI workloads with a range of different model weights. AKS provides the option to deploy
inferencing workloads using open-source presets supported out-of-box and managed in KAITO model registry (https://github.com/kaito-project/kaito/tree/main/presets)
or to dynamically download from the HuggingFace (https://huggingface.co/models) registry at runtime onto your AKS cluster.

Choose an open-source language model from HuggingFace
----------------------------------------------------
In this example, we use the BigScience Bloom-1B7 small language model. Alternatively, you can choose from thousands of text-generation models supported on 
HuggingFace.

1. Connect to your AKS cluster using the az aks get-credentials command: 
Open Cloud Shell > az aks get-credentials --resource-group <resource-group-name> --name <aks-cluster-name>

2. Clone the KAITO project GitHub repository using the git clone command: git clone https://github.com/kaito-project/kaito.git

3. Confirm that your kaito-gpu-provisioner pod is running successfully using the kubectl get deployment command: kubectl get deployment -n kube-system | grep kaito

Deploy your model inferencing workload using the KAITO workspace template
---------------------------------------------------------------------------
1. Navigate to the kaito directory and open the docs/custom-model-integration/reference_image_deployment.yaml KAITO template. Replace the default values in the
following fields with your model's requirements:
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
instanceType: The minimum VM size for this inference service deployment is Standard_NC24ads_A100_v4. For larger model sizes you can choose a VM in the 
Standard_NCads_A100_v4 family with higher memory capacity.
MODEL_ID: Replace with your model's specific HuggingFace identifier, which can be found after https://huggingface.co/ in the model card URL.
"--torch_dtype": Set to "float16" for compatibility with V100 GPUs. For A100, H100 or newer GPUs, use "bfloat16".

apiVersion: kaito.sh/v1alpha1
kind: Workspace
metadata:
  name: workspace-custom-llm
resource:
  instanceType: "Standard_NC24ads_A100_v4"
  labelSelector:
    matchLabels:
      apps: custom-llm
inference:
  template: 
    spec:
      containers:
      - name: custom-llm-container
        image: ghcr.io/kaito-project/kaito/llm-reference-preset:latest
        command: ["accelerate"]
        args:
          - "launch"
          - "--num_processes"
          - "1"
          - "--num_machines"
          - "1"
          - "--gpu_ids"
          - "all"
          - "tfs/inference_api.py"
          - "--pipeline"
          - "text-generation"
          - "--trust_remote_code"
          - "--allow_remote_files"
          - "--pretrained_model_name_or_path"
          - "bigscience/bloom-1b7"
          - "--torch_dtype"
          - "bfloat16"
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory


2. Save these changes to your docs/custom-model-integration/reference_image_deployment.yaml file.

3. Run the deployment in your AKS cluster using the kubectl apply command.
kubectl apply -f docs/custom-model-integration/reference_image_deployment.yaml

Test your custom model inferencing service
---------------------------------------------
1. Track the live resource changes in your KAITO workspace using the kubectl get workspace command: kubectl get workspace workspace-custom-llm -w

** Note that machine readiness can take up to 10 minutes, and workspace readiness up to 20 minutes.

2. Check your language model inference service and get the service IP address using the kubectl get svc command.
export SERVICE_IP=$(kubectl get svc workspace-custom-llm -o jsonpath='{.spec.clusterIP}')

3. Test your custom model inference service with a sample input of your choice using the OpenAI API format:

kubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bloom-1b7",
    "prompt": "What sport should I play in rainy weather?",
    "max_tokens": 20
  }'

Clean up resources
------------------
Delete the KAITO inference workspace using the kubectl delete workspace command: kubectl delete workspace workspace-custom-llm


Monitor and visualize AI inference metrics on Azure Kubernetes Service (AKS) with the AI toolchain operator add-on
==================================================================================================================

Monitoring and observability play a key role in maintaining high performance and low cost of your AI workload deployments in Azure Kubernetes Service (AKS).
Visibility into system and performance metrics can indicate the limits of your underlying infrastructure and motivate real-time adjustments and optimizations
to reduce workload interruptions. Monitoring also provides valuable insights into resource utilization for cost-effective management of computational resources
and accurate provisioning.
The Kubernetes AI Toolchain Operator (KAITO) is a managed add-on for AKS.. 

Deploy a KAITO inference service
---------------------------------
Collect metrics for the Qwen-2.5-coder-7B-instruct language model.

1. Start by applying the following KAITO workspace custom resource to your cluster:
kubectl apply -f https://raw.githubusercontent.com/Azure/kaito/main/examples/inference/kaito_workspace_qwen_2.5_coder_7b-instruct.yaml

2. Track the live resource changes in your KAITO workspace: kubectl get workspace workspace-qwen-2-5-coder-7b-instruct -w

Machine readiness can take up to 10 minutes, and workspace readiness can take up to 20 minutes depending on the size of your language model.

3. Confirm that your inference service is running and get the service IP address:
export SERVICE_IP=$(kubectl get svc workspace-qwen-2-5-coder-7b-instruct -o jsonpath='{.spec.clusterIP}')

echo $SERVICE_IP


Surface KAITO inference metrics to the managed service for Prometheus
----------------------------------------------------------------------
Prometheus metrics are collected by default at the KAITO /metrics endpoint.

1. Add the following label to your KAITO inference service so that a Kubernetes ServiceMonitor deployment can detect it:
kubectl label svc workspace-qwen-2-5-coder-7b-instruct App=qwen-2-5-coder 

2. Create a ServiceMonitor resource to define the inference service endpoints and the required configurations to scrape the vLLM Prometheus metrics. 
Export these metrics to the managed service for Prometheus by deploying the following ServiceMonitor YAML manifest in the kube-system namespace:

cat <<EOF | kubectl apply -n kube-system -f -
apiVersion: azmonitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-kaito-monitor
spec:
  selector:
    matchLabels:
      App: qwen-2-5-coder
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
    scheme: http
EOF

Check for the following output to verify that ServiceMonitor is created:Output

servicemonitor.azmonitoring.coreos.com/prometheus-kaito-monitor created

3. Verify that your ServiceMonitor deployment is running successfully: kubectl get servicemonitor prometheus-kaito-monitor -n kube-system

4. In the Azure portal, verify that vLLM metrics are successfully collected in the managed service for Prometheus.

4a. In your Azure Monitor workspace, go to Managed Prometheus > Prometheus explorer.

4b. Select the Grid tab and confirm that a metrics item is associated with the job named workspace-qwen-2-5-coder-7b-instruct.

Visualize KAITO inference metrics in Azure Managed Grafana
----------------------------------------------------------
1. The vLLM project provides a Grafana dashboard configuration named grafana.json for inference workload monitoring. Navigate to the bottom of this page and copy
the entire contents of the grafana.json file.

2. Go to the bottom of the examples page and copy the entire contents of the grafana.json file:

3. Complete the steps to import the Grafana configurations into a new dashboard in Azure Managed Grafana.

4. Go to your Managed Grafana endpoint, view the available dashboards, and select the vLLM dashboard.

5. To begin collecting data for your selected model deployment, confirm that the datasource value shown at the top left of the Grafana dashboard is your instance
of the managed service for Prometheus you created for this example.

6. Copy the inference preset name defined in your KAITO workspace to the model_name field in the Grafana dashboard. In this example, the model name is 
qwen2.5-coder-7b-instruct.

7. In a few moments, verify that the metrics for your KAITO inference service appear in the vLLM Grafana dashboard.























































































































































